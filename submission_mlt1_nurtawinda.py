# -*- coding: utf-8 -*-
"""Submission_MLT1_Nurtawinda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dviliTA6yGGCqOQjX2mfBblC98HWGhoo

# Import Library dan Load Data
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import os

# load the dataset
house = pd.read_csv('/content/datasheet/House Price Prediction Dataset.csv')
house.head()

"""# EDA"""

house.info()

house.describe()

house.drop(columns=['Id'], inplace=True)

sns.boxplot(x=house['Area'])

sns.boxplot(x=house['Bedrooms'])

sns.boxplot(x=house['Bathrooms'])

sns.boxplot(x=house['Floors'])

sns.boxplot(x=house['YearBuilt'])

sns.boxplot(x=house['Price'])

numerical_features = ['Area', 'Bedrooms', 'Bathrooms', 'Floors', 'YearBuilt', 'Price']
categorical_features = ['Location', 'Condition', 'Garage']

feature = categorical_features[0]
count = house[feature].value_counts()
percent = 100*house[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature)

feature = categorical_features[1]
count = house[feature].value_counts()
percent = 100*house[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature)

feature = categorical_features[2]
count = house[feature].value_counts()
percent = 100*house[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature)

house.hist(bins=50, figsize=(20,15))
plt.show()

cat_features = house.select_dtypes(include='object').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y="Price", kind="bar", dodge=False, height = 4, aspect = 3,  data=house, palette="Set3")
  plt.title("Rata-rata 'price' Relatif terhadap - {}".format(col))

"""### Penilaian Korelasi antar feature numerik"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(house, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = house[numerical_features].corr().round(2)

# Untuk mencetak nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""*Insight*
Data yang digunakan tidak menunjukkan adanya outlier, data hilang, maupun data yang terduplikat. Fitur kategorikal hampir semuanya memiliki nilai rata-rata yang hampir sama begitupun dengan fitur numerik. Dari matrik korelasi, diketahui bahwa Bedrooms, Bathrooms, dan Floors memiliki korelasi yang cukup tinggi dengan area rumah.

## Encoding feature
"""

from sklearn.preprocessing import  OneHotEncoder
house = pd.concat([house, pd.get_dummies(house['Location'], prefix='Location')],axis=1)
house = pd.concat([house, pd.get_dummies(house['Condition'], prefix='Condition')],axis=1)
house = pd.concat([house, pd.get_dummies(house['Garage'], prefix='Garage')],axis=1)
house.drop(['Location','Condition','Garage'], axis=1, inplace=True)
house.head()

"""*Insight*
Pada bagian ini, dilakukan encoding dengan OneHotEncoder sehingga nilai dari fitur kategorikal akakn diubah menjadi nilai biner. Ini akan memudahkan model dalam mengolah data

## PCA
"""

sns.pairplot(house[['Bedrooms','Bathrooms','Floors']], plot_kws={"s": 3})

from sklearn.decomposition import PCA

pca = PCA(n_components=3, random_state=123)
pca.fit(house[['Bedrooms','Bathrooms','Floors']])
princ_comp = pca.transform(house[['Bedrooms','Bathrooms','Floors']])

pca.explained_variance_ratio_.round(3)

from sklearn.decomposition import PCA
pca = PCA(n_components=1, random_state=123)
pca.fit(house[['Bedrooms','Bathrooms','Floors']])
house['Area'] = pca.transform(house.loc[:, ('Bedrooms','Bathrooms','Floors')]).flatten()
house.drop(['Bedrooms','Bathrooms','Floors'], axis=1, inplace=True)

"""## Pembagian dataset"""

from sklearn.model_selection import train_test_split

X = house.drop(["Price"],axis =1)
y = house["Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

from sklearn.preprocessing import StandardScaler

numerical_features = ['Area', 'YearBuilt', 'Location_Downtown', 'Location_Rural',
       'Location_Suburban', 'Location_Urban', 'Condition_Excellent',
       'Condition_Fair', 'Condition_Good', 'Condition_Poor', 'Garage_No',
       'Garage_Yes']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

X_train[numerical_features].describe().round(4)

"""*Insight* : Proporsi pembagian data latih dan uji adalah 90:10. Sehingga data latih berjumlah 1800 data dan 200 sisanya merupakan data uji.

# Pemodelan
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)

models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

# Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor

# buat model prediksi
RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)

models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

from sklearn.ensemble import AdaBoostRegressor

boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

"""# Evaluasi"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

prediksi = X_test.iloc[:5].copy()
pred_dict = {'y_true':y_test[:5]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""*Insight*
model Boosting menunjukkan kinerja yang paling baik dalam hal generalisasi dibandingkan KNN dan RandomForest. Meskipun nilai MSE Boosting pada data train tidak serendah RandomForest, perbedaan antara MSE pada data train dan test untuk Boosting jauh lebih kecil. Ini mengindikasikan bahwa model Boosting mampu mempertahankan performanya dengan lebih baik ketika dihadapkan pada data yang belum pernah dilihat sebelumnya (data test).
"""